{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "test_tam = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grasp = pd.read_csv('data-optical-design (7).csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colunas = ['x','y','z','theta','phi']\n",
    "data = pd.read_csv('data-optical-design (7).csv', usecols= colunas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler1 = StandardScaler()\n",
    "scaler2 = StandardScaler()\n",
    "data[['x','y']] = scaler1.fit_transform(data[['x','y']])\n",
    "data[['z']] = scaler2.fit_transform(data[['z']])\n",
    "X_z = data[['x','y']]\n",
    "y_z = data['z']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_z = ['x','y']\n",
    "colunas_z = [tf.feature_column.numeric_column(key = c) for c in col_z]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep = 300\n",
    "for j in tqdm(range(rep)):\n",
    "    colunas = ['x','y','z','theta','phi']\n",
    "    data = pd.read_csv('data-optical-design (7).csv', usecols= colunas)\n",
    "    \n",
    "    \n",
    "    scaler1 = StandardScaler()\n",
    "    scaler2 = StandardScaler()\n",
    "    data[['x','y']] = scaler1.fit_transform(data[['x','y']])\n",
    "    data[['z']] = scaler2.fit_transform(data[['z']])\n",
    "    X_z = data[['x','y']]\n",
    "    y_z = data['z']\n",
    "\n",
    "    \n",
    "    X_train_z,X_test_z,y_train_z,y_test_z = train_test_split(X_z,y_z,test_size = test_tam)\n",
    "    \n",
    "    \n",
    "    fn_z = tf.compat.v1.estimator.inputs.pandas_input_fn(x = X_train_z, y = y_train_z, batch_size= 8,\n",
    "                                                   num_epochs= None, shuffle= True)\n",
    "    \n",
    "    reg_z = tf.estimator.DNNRegressor(hidden_units=[10], feature_columns=colunas_z,\n",
    "                                  activation_fn=tf.keras.activations.relu,optimizer= lambda: tf.keras.optimizers.SGD(\n",
    "        learning_rate=tf.compat.v1.train.exponential_decay(\n",
    "            learning_rate=0.1,\n",
    "            global_step=tf.compat.v1.train.get_global_step(),\n",
    "            decay_steps=10000,\n",
    "            decay_rate=0.96)))\n",
    "    \n",
    "    reg_z.train(input_fn = fn_z, steps = 20000)\n",
    "    fn_prev_z = tf.compat.v1.estimator.inputs.pandas_input_fn(x = X_test_z, shuffle = False)\n",
    "    \n",
    "    val_prev_z= []\n",
    "    for p in reg_z.predict(input_fn=fn_prev_z):\n",
    "        val_prev_z.append(p['predictions'][0])\n",
    "        \n",
    "    aux = np.asarray(scaler2.inverse_transform(val_prev_z)).reshape(-1,1)\n",
    "    np.save('data_output/data_ypre_test_z'+ str(j)+'.npy',aux)\n",
    "   \n",
    "    fn_prev_z = tf.compat.v1.estimator.inputs.pandas_input_fn(x = X_train_z, shuffle = False)\n",
    "    \n",
    "    val_prev_z= []\n",
    "    for p in reg_z.predict(input_fn=fn_prev_z):\n",
    "        val_prev_z.append(p['predictions'][0])\n",
    "    \n",
    "    aux = np.asarray(scaler2.inverse_transform(val_prev_z)).reshape(-1,1)\n",
    "    np.save('data_output/data_ypre_train_z'+ str(j)+'.npy',aux)    \n",
    "    \n",
    "    fn_prev_z = tf.compat.v1.estimator.inputs.pandas_input_fn(x = X_z, shuffle = False)\n",
    "\n",
    "    \n",
    "    val_prev_z= []\n",
    "    for p in reg_z.predict(input_fn=fn_prev_z):\n",
    "        val_prev_z.append(p['predictions'][0])\n",
    "    \n",
    "    \n",
    "    aux = np.asarray(scaler2.inverse_transform(val_prev_z)).reshape(-1,1)\n",
    "    np.save('data_output/data_ypre_total_z'+ str(j)+'.npy',aux)\n",
    "    \n",
    "    X_train_z = scaler1.inverse_transform(X_train_z)\n",
    "    y_train_z = scaler2.inverse_transform(y_train_z)\n",
    "    X_test_z = scaler1.inverse_transform(X_test_z)\n",
    "    y_test_z = scaler2.inverse_transform(y_test_z)\n",
    "    \n",
    "    \n",
    "    np.save('data_output/data_xtrain_z'+ str(j)+'.npy',X_train_z)\n",
    "    np.save('data_output/data_ytrain_z'+ str(j)+'.npy',y_train_z.reshape(-1,1))\n",
    "    np.save('data_output/data_xtest_z'+ str(j)+'.npy',X_test_z)\n",
    "    np.save('data_output/data_ytest_z'+ str(j)+'.npy',y_test_z.reshape(-1,1))\n",
    "    \n",
    "    #X_train_z.to_csv('data_output/data_xtrain_z'+ str(j)+'.csv')\n",
    "    #y_train_z.to_csv('data_output/data_ytrain_z'+ str(j)+'.csv',header = True)\n",
    "    #X_test_z.to_csv('data_output/data_xtest_z'+ str(j)+'.csv')\n",
    "    #y_test_z.to_csv('data_output/data_ytest_z'+ str(j)+'.csv', header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit $\\theta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colunas = ['x','y','z','theta','phi']\n",
    "data = pd.read_csv('data-optical-design (7).csv', usecols= colunas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler3 = StandardScaler()\n",
    "scaler4 = StandardScaler()\n",
    "data[['x','y','z']] = scaler3.fit_transform(data[['x','y','z']])\n",
    "data[['theta']] = scaler4.fit_transform(data[['theta']])\n",
    "X_theta = data[['x','y','z']]\n",
    "y_theta = data['theta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_theta = ['x','y','z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colunas_theta = [tf.feature_column.numeric_column(key = c) for c in col_theta]\n",
    "colunas_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep = 300\n",
    "for j in tqdm(range(rep)):\n",
    "    \n",
    "    colunas = ['x','y','z','theta','phi']\n",
    "    data = pd.read_csv('data-optical-design (6).csv', usecols= colunas)\n",
    "\n",
    "    scaler3 = StandardScaler()\n",
    "    scaler4 = StandardScaler()\n",
    "    data[['x','y','z']] = scaler3.fit_transform(data[['x','y','z']])\n",
    "    data[['theta']] = scaler4.fit_transform(data[['theta']])\n",
    "    X_theta = data[['x','y','z']]\n",
    "    y_theta = data['theta']\n",
    "\n",
    "\n",
    "    X_train_t,X_test_t,y_train_t,y_test_t = train_test_split(X_theta,y_theta, test_size = test_tam)\n",
    "\n",
    "\n",
    "    fn_theta = tf.compat.v1.estimator.inputs.pandas_input_fn(x = X_train_t, y = y_train_t, batch_size= 8,\n",
    "                                                   num_epochs= None, shuffle= True)\n",
    "    reg_theta = tf.estimator.DNNRegressor(hidden_units=[10], feature_columns=colunas_theta,\n",
    "                                  activation_fn=tf.nn.relu, optimizer='Adam')\n",
    "    reg_theta.train(input_fn = fn_theta, steps = 20000)\n",
    "\n",
    "    fn_prev_theta = tf.compat.v1.estimator.inputs.pandas_input_fn(x = X_test_t, shuffle = False)\n",
    "\n",
    "    val_prev_theta= []\n",
    "    for p in reg_theta.predict(input_fn=fn_prev_theta):\n",
    "        val_prev_theta.append(p['predictions'][0])\n",
    "    aux = np.asarray(scaler4.inverse_transform(val_prev_theta)).reshape(-1,1)\n",
    "    np.save('data_output/data_ypre_test_theta'+ str(j)+'.npy',aux)\n",
    "\n",
    "\n",
    "    fn_prev_theta = tf.compat.v1.estimator.inputs.pandas_input_fn(x = X_train_t, shuffle = False)\n",
    "\n",
    "    val_prev_theta= []\n",
    "    for p in reg_theta.predict(input_fn=fn_prev_theta):\n",
    "        val_prev_theta.append(p['predictions'][0])\n",
    "    aux = np.asarray(scaler4.inverse_transform(val_prev_theta)).reshape(-1,1)\n",
    "    np.save('data_output/data_ypre_train_theta'+ str(j)+'.npy',aux)\n",
    "\n",
    "    fn_prev_theta = tf.compat.v1.estimator.inputs.pandas_input_fn(x = X_theta, shuffle = False)\n",
    "\n",
    "    val_prev_theta= []\n",
    "    for p in reg_theta.predict(input_fn=fn_prev_theta):\n",
    "        val_prev_theta.append(p['predictions'][0])\n",
    "    aux = np.asarray(scaler4.inverse_transform(val_prev_theta)).reshape(-1,1)\n",
    "\n",
    "    np.save('data_output/data_ypre_total_theta'+ str(j)+'.npy',aux)\n",
    "\n",
    "    X_train_t = scaler3.inverse_transform(X_train_t)\n",
    "    y_train_t = scaler4.inverse_transform(y_train_t)\n",
    "    X_test_t = scaler3.inverse_transform(X_test_t)\n",
    "    y_test_t = scaler4.inverse_transform(y_test_t)\n",
    "\n",
    "    np.save('data_output/data_xtrain_theta'+ str(j)+'.npy',X_train_t)\n",
    "    np.save('data_output/data_ytrain_theta'+ str(j)+'.npy',y_train_t.reshape(-1,1))\n",
    "    np.save('data_output/data_xtest_theta'+ str(j)+'.npy',X_test_t)\n",
    "    np.save('data_output/data_ytest_theta'+ str(j)+'.npy',y_test_t.reshape(-1,1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler5 = StandardScaler()\n",
    "scaler6 = StandardScaler()\n",
    "data[['x','y','theta']] = scaler5.fit_transform(data[['x','y','theta']])\n",
    "data[['phi']] = scaler6.fit_transform(data[['phi']])\n",
    "phi = np.abs(data['phi'].values).reshape(-1,1)\n",
    "data['phi'] = phi\n",
    "X_phi = data[['x','y','theta']]\n",
    "y_phi = data['phi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_phi = ['x','y','theta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colunas_phi = [tf.feature_column.numeric_column(key = c) for c in col_phi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep = 300\n",
    "for j in tqdm(range(rep)):\n",
    "    \n",
    "    colunas = ['x','y','z','theta','phi']\n",
    "    data = pd.read_csv('data-optical-design (7).csv', usecols= colunas)\n",
    "    #phi = np.abs(data['phi'].values).reshape(-1,1)\n",
    "    #data['phi'] = phi\n",
    "    \n",
    "    scaler5 = StandardScaler()\n",
    "    scaler6 = StandardScaler()\n",
    "    data[['x','y','theta']] = scaler5.fit_transform(data[['x','y','theta']])\n",
    "    data[['phi']] = scaler6.fit_transform(data[['phi']])\n",
    "    X_phi = data[['x','y','theta']]\n",
    "    y_phi = data['phi']\n",
    "    \n",
    "    \n",
    "    X_train_phi,X_test_phi,y_train_phi,y_test_phi = train_test_split(X_phi,y_phi,test_size = test_tam)\n",
    "    \n",
    "    fn_phi = tf.compat.v1.estimator.inputs.pandas_input_fn(x = X_train_phi, y = y_train_phi, batch_size= 8,\n",
    "                                                   num_epochs= None, shuffle= True)\n",
    "\n",
    "    reg_phi = tf.estimator.DNNRegressor(hidden_units=[10],\n",
    "                                    feature_columns=colunas_phi,\n",
    "                                    activation_fn=tf.keras.activations.tanh,\n",
    "                                    optimizer= lambda: tf.keras.optimizers.SGD(\n",
    "                                    learning_rate=\n",
    "                                    tf.compat.v1.train.exponential_decay(\n",
    "                                                            learning_rate=0.1,\n",
    "                                global_step=tf.compat.v1.train.get_global_step(),\n",
    "                                                                decay_steps=10000,\n",
    "            decay_rate=0.96)))\n",
    "    \n",
    "    reg_phi.train(input_fn = fn_phi, steps = 20000)\n",
    "\n",
    "\n",
    "    fn_prev_phi = tf.compat.v1.estimator.inputs.pandas_input_fn(x = X_test_phi, shuffle = False)\n",
    "\n",
    "    \n",
    "    val_prev_phi= []\n",
    "    for p in reg_phi.predict(input_fn=fn_prev_phi):\n",
    "        val_prev_phi.append(p['predictions'][0])\n",
    "    aux = np.asarray(scaler6.inverse_transform(val_prev_phi)).reshape(-1,1)\n",
    "    np.save('data_output/data_ypre_test_phi'+ str(j)+'.npy',aux)\n",
    "\n",
    "    fn_prev_phi = tf.compat.v1.estimator.inputs.pandas_input_fn(x = X_train_phi, shuffle = False)\n",
    "\n",
    "    \n",
    "    val_prev_phi= []\n",
    "    for p in reg_phi.predict(input_fn=fn_prev_phi):\n",
    "        val_prev_phi.append(p['predictions'][0])\n",
    "    aux = np.asarray(scaler6.inverse_transform(val_prev_phi)).reshape(-1,1)\n",
    "    np.save('data_output/data_ypre_train_phi'+ str(j)+'.npy',aux)\n",
    "    \n",
    "    fn_prev_phi = tf.compat.v1.estimator.inputs.pandas_input_fn(x = X_phi, shuffle = False)\n",
    "\n",
    "    val_prev_phi = []\n",
    "    for p in reg_phi.predict(input_fn=fn_prev_phi):\n",
    "        val_prev_phi.append(p['predictions'][0])\n",
    "    aux = np.asarray(scaler6.inverse_transform(val_prev_phi)).reshape(-1,1)\n",
    "   \n",
    "    np.save('data_output/data_ypre_total_phi'+ str(j)+'.npy',aux)    \n",
    "    \n",
    "    \n",
    "    X_train_phi = scaler5.inverse_transform(X_train_phi)\n",
    "    y_train_phi = scaler6.inverse_transform(y_train_phi)\n",
    "    X_test_phi = scaler5.inverse_transform(X_test_phi)\n",
    "    y_test_phi = scaler6.inverse_transform(y_test_phi)\n",
    "    \n",
    "    np.save('data_output/data_xtrain_phi'+ str(j)+'.npy', X_train_phi)\n",
    "    np.save('data_output/data_ytrain_phi'+ str(j)+'.npy',y_train_phi.reshape(-1,1))\n",
    "    np.save('data_output/data_xtest_phi'+ str(j)+'.npy',X_test_phi)\n",
    "    np.save('data_output/data_ytest_phi'+ str(j)+'.npy',y_test_phi.reshape(-1,1))\n",
    "    \n",
    "    \n",
    "    #X_train_phi.to_csv('data_output/data_xtrain_phi'+ str(j)+'.csv')\n",
    "    #y_train_phi.to_csv('data_output/data_ytrain_phi'+ str(j)+'.csv',header = True)\n",
    "    #X_test_phi.to_csv('data_output/data_xtest_phi'+ str(j)+'.csv')\n",
    "    #y_test_phi.to_csv('data_output/data_ytest_phi'+ str(j)+'.csv', header = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
